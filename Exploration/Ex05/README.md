# AIFFEL Campus Online Code Peer Review Templete
- 코더 : 홍사빈
- 리뷰어 : 김민혁


# PRT(Peer Review Template)
- [X]  **1. 주어진 문제를 해결하는 완성된 코드가 제출되었나요?**
    - 문제에서 요구하는 최종 결과물이 첨부되었는지 확인
        - 중요! 해당 조건을 만족하는 부분을 캡쳐해 근거로 첨부
     
    ![image](https://github.com/user-attachments/assets/f6714958-e8ad-4678-8554-941b76a62a77)
    CNN 모델을 Conv1D 층 사이마다 BatchNormalization을 넣고 GlobalmaxPooling한 점이 좋았다. 또 CNN 뿐만 아닌 LSTM 모델 등 다양한 모델로 Word2Vec을 시도한 점이 흥미로웠다.
    
- [X]  **2. 전체 코드에서 가장 핵심적이거나 가장 복잡하고 이해하기 어려운 부분에 작성된 
주석 또는 doc string을 보고 해당 코드가 잘 이해되었나요?**
    - 해당 코드 블럭을 왜 핵심적이라고 생각하는지 확인
    - 해당 코드 블럭에 doc string/annotation이 달려 있는지 확인
    - 해당 코드의 기능, 존재 이유, 작동 원리 등을 기술했는지 확인
    - 주석을 보고 코드 이해가 잘 되었는지 확인
        - 중요! 잘 작성되었다고 생각되는 부분을 캡쳐해 근거로 첨부
    
    ![image](https://github.com/user-attachments/assets/cdfda375-4db2-4d60-89b5-454e47a4fcc7)
    Gensim을 사용하는 부분에서 주석 설명을 통해 어떠한 과정으로 학습이 되고 비교 분석이 되는지 그 과정을 잘 알 수 있었다.     
        
- [X]  **3. 에러가 난 부분을 디버깅하여 문제를 해결한 기록을 남겼거나
새로운 시도 또는 추가 실험을 수행해봤나요?**
    - 문제 원인 및 해결 과정을 잘 기록하였는지 확인
    - 프로젝트 평가 기준에 더해 추가적으로 수행한 나만의 시도, 
    실험이 기록되어 있는지 확인
        - 중요! 잘 작성되었다고 생각되는 부분을 캡쳐해 근거로 첨부

    ![image](https://github.com/user-attachments/assets/f4a58caf-8ef1-49ed-adf5-4df40718b5ad)
    LSTM 모델을 학습할 때 바로 에폭이 시작되지 않고 WARNING 메시지에 대한 설명을 통해 에러의 판별이 되는지 아닌지를 잘 구분하였다. 결과의 정확도에 영향을 줄 수는 있으나 넘어가도 되는 부분이기에 혹시 직접 실행해본다면 놀라지 않아도 된다는 뜻을 잘 받았다.
    
        
- [X]  **4. 회고를 잘 작성했나요?**
    - 주어진 문제를 해결하는 완성된 코드 내지 프로젝트 결과물에 대해
    배운점과 아쉬운점, 느낀점 등이 기록되어 있는지 확인
    - 전체 코드 실행 플로우를 그래프로 그려서 이해를 돕고 있는지 확인
        - 중요! 잘 작성되었다고 생각되는 부분을 캡쳐해 근거로 첨부

    ![image](https://github.com/user-attachments/assets/6c0577e5-b236-4fa2-8911-6bcfd39f1fb6)
    accuracy를 높이기 위해 에폭을 오래 기다린 흔적이 보였다. 꼭 알맞는 에폭 크기를 찾아서 정확도를 높이는 것에 응원하게 만든 회고였다:)
     
    
        
- [X]  **5. 코드가 간결하고 효율적인가요?**
    - 파이썬 스타일 가이드 (PEP8) 를 준수하였는지 확인
    - 코드 중복을 최소화하고 범용적으로 사용할 수 있도록 함수화/모듈화했는지 확인
        - 중요! 잘 작성되었다고 생각되는 부분을 캡쳐해 근거로 첨부
     
    ![image](https://github.com/user-attachments/assets/4b00e703-922e-42cd-8a74-c94c376fddd6)
    Transformer 인코더 부분을 함수화를 통해 깔끔하게 정리했고, Call, 트랜스포머 모델 함수도 세세한 모듈화로 잘 구분해 놓을 수 있는 부분에 칭찬을 드린다. 

# 회고(참고 링크 및 코드 개선)
```
LSTM, CNN, RNN 다양한 여러 모델로 수행하며 Word2Vec 도 시도한 것이 인상 깊었다.
특히 트랜스포머 인코더, 모델, early Stop 기법을 통해 다양한 시도로 좋은 성능을 찾으려는 모습이 보기 좋았다.
학습 때마다 열심히 잘 하시고 탐구하고 남들보다 다른 시도를 해본다는 것에 항상 칭찬을 드린다:)
```

